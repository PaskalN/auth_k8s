apiVersion: v1
data:
  broker.properties: |
    # Licensed to the Apache Software Foundation (ASF) under one or more
    # contributor license agreements.  See the NOTICE file distributed with
    # this work for additional information regarding copyright ownership.
    # The ASF licenses this file to You under the Apache License, Version 2.0
    # (the "License"); you may not use this file except in compliance with
    # the License.  You may obtain a copy of the License at
    #
    #    http://www.apache.org/licenses/LICENSE-2.0
    #
    # Unless required by applicable law or agreed to in writing, software
    # distributed under the License is distributed on an "AS IS" BASIS,
    # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    # See the License for the specific language governing permissions and
    # limitations under the License.

    ############################# Server Basics #############################

    # The role of this server. Setting this puts us in KRaft mode
    # Must be |process.roles=broker|
    process.roles=((KAFKA_PROCESS_ROLES))

    # The node id associated with this instance's roles
    node.id=((KAFKA_NODE_ID))

    # Information about the KRaft controller quorum.
    controller.quorum.bootstrap.servers=((KAFKA_CONTROLLER_QUORUM_BOOTSTRAP_SERVERS))

    ############################# Socket Server Settings #############################

    # The address the socket server listens on. If not configured, the host name will be equal to the value of
    # java.net.InetAddress.getCanonicalHostName(), with PLAINTEXT listener name, and port 9092.
    #   FORMAT:
    #     listeners = listener_name://host_name:port
    #   EXAMPLE:
    #     listeners = PLAINTEXT://your.host.name:9092
    listeners=((KAFKA_LISTENERS))

    # Name of listener used for communication between brokers.
    inter.broker.listener.name=PLAINTEXT

    # Listener name, hostname and port the broker will advertise to clients.
    # If not set, it uses the value for "listeners".
    advertised.listeners=((KAFKA_ADVERTISED_LISTENERS))

    # A comma-separated list of the names of the listeners used by the controller.
    # This is required if running in KRaft mode. On a node with `process.roles=broker`, only the first listed listener will be used by the broker.
    controller.listener.names=((KAFKA_CONTROLLER_LISTENER_NAMES))

    # Maps listener names to security protocols, the default is for them to be the same. See the config documentation for more details
    listener.security.protocol.map=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL

    # The number of threads that the server uses for receiving requests from the network and sending responses to the network
    num.network.threads=((KAFKA_NUM_NETWORK_THREADS))

    # The number of threads that the server uses for processing requests, which may include disk I/O
    num.io.threads=((KAFKA_NUM_IO_THREADS))

    # The send buffer (SO_SNDBUF) used by the socket server
    socket.send.buffer.bytes=((KAFKA_SOCKET_SEND_BUFFER_BYTES))

    # The receive buffer (SO_RCVBUF) used by the socket server
    socket.receive.buffer.bytes=((KAFKA_RECIEVE_BUFFER_BYTES))

    # The maximum size of a request that the socket server will accept (protection against OOM)
    socket.request.max.bytes=((KAFKA_REQUEST_MAX_BYTES))


    ############################# Log Basics #############################

    # A comma separated list of directories under which to store log files
    log.dirs=((KAFKA_LOG_DIR))

    # The default number of log partitions per topic. More partitions allow greater
    # parallelism for consumption, but this will also result in more files across
    # the brokers.
    num.partitions=((KAFKA_NUM_PARTITIONS))

    # The number of threads per data directory to be used for log recovery at startup and flushing at shutdown.
    # This value is recommended to be increased for installations with data dirs located in RAID array.
    num.recovery.threads.per.data.dir=((KAFKA_RECOVERY_THREADS_PER_DATA_DIR))

    ############################# Internal Topic Settings  #############################
    # The replication factor for the group metadata internal topics "__consumer_offsets" and "__transaction_state"
    # For anything other than development testing, a value greater than 1 is recommended to ensure availability such as 3.
    offsets.topic.replication.factor=((KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR))
    transaction.state.log.replication.factor=((KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR))
    transaction.state.log.min.isr=((KAFKA_TRANSACTION_STATE_LOG_MIN_ISR))

    # Share state topic settings
    share.coordinator.state.topic.replication.factor=((KAFKA_SHARE_COORDINATOR_STATE_TOPIC_REPLICATION_FACTOR))
    share.coordinator.state.topic.min.isr=((KAFKA_SHARE_COORDINATOR_STATE_TOPIC_MIN_ISR))

    ############################# Log Flush Policy #############################

    # Messages are immediately written to the filesystem but by default we only fsync() to sync
    # the OS cache lazily. The following configurations control the flush of data to disk.
    # There are a few important trade-offs here:
    #    1. Durability: Unflushed data may be lost if you are not using replication.
    #    2. Latency: Very large flush intervals may lead to latency spikes when the flush does occur as there will be a lot of data to flush.
    #    3. Throughput: The flush is generally the most expensive operation, and a small flush interval may lead to excessive seeks.
    # The settings below allow one to configure the flush policy to flush data after a period of time or
    # every N messages (or both). This can be done globally and overridden on a per-topic basis.

    # The number of messages to accept before forcing a flush of data to disk
    #log.flush.interval.messages=10000

    # The maximum amount of time a message can sit in a log before we force a flush
    #log.flush.interval.ms=1000

    ############################# Log Retention Policy #############################

    # The following configurations control the disposal of log segments. The policy can
    # be set to delete segments after a period of time, or after a given size has accumulated.
    # A segment will be deleted whenever *either* of these criteria are met. Deletion always happens
    # from the end of the log.

    # The minimum age of a log file to be eligible for deletion due to age
    log.retention.hours=((KAFKA_RETENTION_HOURS))

    # A size-based retention policy for logs. Segments are pruned from the log unless the remaining
    # segments drop below log.retention.bytes. Functions independently of log.retention.hours.
    #log.retention.bytes=1073741824

    # The maximum size of a log segment file. When this size is reached a new log segment will be created.
    log.segment.bytes=((KAFKA_LOG_SEGMENT_BYTES))

    # The interval at which log segments are checked to see if they can be deleted according
    # to the retention policies
    log.retention.check.interval.ms=((KAFKA_RETENTION_CHECK_INTERVAL_MS))
  broker_server_properties.sh: |-
    #!/bin/sh

    set -e

    generate_broker_server_file() {
        if [ "$KAFKA_PROCESS_ROLES" = "broker" ]; then
            sed -e "s#((KAFKA_NODE_ID))#$KAFKA_NODE_ID#g" \
                -e "s#((KAFKA_LOG_DIR))#$KAFKA_LOG_DIR#g" \
                -e "s#((KAFKA_LISTENERS))#$KAFKA_LISTENERS#g" \
                -e "s#((KAFKA_PROCESS_ROLES))#$KAFKA_PROCESS_ROLES#g" \
                -e "s#((KAFKA_CONTROLLER_LISTENER_NAMES))#$KAFKA_CONTROLLER_LISTENER_NAMES#g" \
                -e "s#((KAFKA_CLUSTER_ID))#$KAFKA_CLUSTER_ID#g" \
                -e "s#((KAFKA_CONTROLLER_QUORUM_VOTERS))#$KAFKA_CONTROLLER_QUORUM_VOTERS#g" \
                -e "s#((KAFKA_ADVERTISED_LISTENERS))#$KAFKA_ADVERTISED_LISTENERS#g" \
                -e "s#((KAFKA_CONTROLLER_QUORUM_BOOTSTRAP_SERVERS))#$KAFKA_CONTROLLER_QUORUM_BOOTSTRAP_SERVERS#g" \
                -e "s#((KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR))#${KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR}#g" \
                -e "s#((KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR))#${KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR}#g" \
                -e "s#((KAFKA_TRANSACTION_STATE_LOG_MIN_ISR))#${KAFKA_TRANSACTION_STATE_LOG_MIN_ISR}#g" \
                -e "s#((KAFKA_NUM_NETWORK_THREADS))#${KAFKA_NUM_NETWORK_THREADS}#g" \
                -e "s#((KAFKA_NUM_IO_THREADS))#${KAFKA_NUM_IO_THREADS}#g" \
                -e "s#((KAFKA_SOCKET_SEND_BUFFER_BYTES))#${KAFKA_SOCKET_SEND_BUFFER_BYTES}#g" \
                -e "s#((KAFKA_RECIEVE_BUFFER_BYTES))#${KAFKA_RECIEVE_BUFFER_BYTES}#g" \
                -e "s#((KAFKA_REQUEST_MAX_BYTES))#${KAFKA_REQUEST_MAX_BYTES}#g" \
                -e "s#((KAFKA_NUM_PARTITIONS))#${KAFKA_NUM_PARTITIONS}#g" \
                -e "s#((KAFKA_RECOVERY_THREADS_PER_DATA_DIR))#${KAFKA_RECOVERY_THREADS_PER_DATA_DIR}#g" \
                -e "s#((KAFKA_SHARE_COORDINATOR_STATE_TOPIC_REPLICATION_FACTOR))#${KAFKA_SHARE_COORDINATOR_STATE_TOPIC_REPLICATION_FACTOR}#g" \
                -e "s#((KAFKA_SHARE_COORDINATOR_STATE_TOPIC_MIN_ISR))#${KAFKA_SHARE_COORDINATOR_STATE_TOPIC_MIN_ISR}#g" \
                -e "s#((KAFKA_RETENTION_HOURS))#${KAFKA_RETENTION_HOURS}#g" \
                -e "s#((KAFKA_RETENTION_CHECK_INTERVAL_MS))#${KAFKA_RETENTION_CHECK_INTERVAL_MS}#g" \
                -e "s#((KAFKA_LOG_SEGMENT_BYTES))#${KAFKA_LOG_SEGMENT_BYTES}#g" \
                "$USER_CONFIG_DIR/commando/broker.properties" \
                > "$USER_CONFIG_DIR/server.properties"
        fi
    }
  commando.sh: |-
    #!/bin/sh

    set -e

    if [ -z "$USER_CONFIG_DIR" ]; then
        print_red "USER_CONFIG_DIR is not defined" 1
        exit 1
    fi

    . "${USER_CONFIG_DIR}/commando/include.sh"

    environment_checker

    generate_controller_server_file

    generate_broker_server_file

    debug

    run_kafka
  commando_broker_members.sh: |-
    #!/bin/sh

    set -e


    # CONTROLLER SETUP
    # POD_NAME CHECKER
    if [ -z "$POD_NAME_CONTROLLER" ]; then
        print_red "POD_NAME_CONTROLLER is missing: Please privide value!" 1
        exit 1
    fi

    # SERVICE_NAME CHECKER
    if [ -z "$SERVICE_NAME" ]; then
        print_red "SERVICE_NAME is missing: Please privide value!" 1
        exit 1
    fi

    # NAMESPACE CHECKER
    if [ -z "$NAMESPACE" ]; then
        print_red "NAMESPACE is missing: Please privide value!" 1
        exit 1
    fi

    # CONTROLLER_NAMESPACE CHECKER
    if [ -z "$CONTROLLER_NAMESPACE" ]; then
        print_red "CONTROLLER_NAMESPACE is missing: Please privide value!" 1
        exit 1
    fi

    # CONTROLLER_REPLICAS CHECKER
    if [ -z "$CONTROLLER_REPLICAS" ]; then
        print_red "CONTROLLER_REPLICAS is missing: Please privide value!" 1
        exit 1
    fi

    # CONTROLLER_SERVICE CHECKER
    if [ -z "$CONTROLLER_SERVICE" ]; then
        print_red "CONTROLLER_SERVICE is missing: Please privide value!" 1
        exit 1
    fi

    KAFKA_CONTROLLER_SET_MEMBERS=$(
      awk -v replicas="$CONTROLLER_REPLICAS" -v node_name="$POD_NAME_CONTROLLER" -v service_name="$CONTROLLER_SERVICE" -v namespace="$CONTROLLER_NAMESPACE" '
        BEGIN {
            sep = ""
            for (i = 0; i < replicas; i++) {
                printf("%s%d@%s-%d.%s.%s.svc.cluster.local:9093", sep, i+1, node_name, i, service_name, namespace)
                sep = ","
            }
        }
      '
    )

    export KAFKA_CONTROLLER_SET_MEMBERS

    # BROKER SETUP
    KAFKA_NODE_ID=$(( ${POD_NAME_INDEX##*-} + 1 ))
    POD_NAME="${POD_NAME_INDEX%-[0-9]*}"

    if [ -z "$POD_NAME" ]; then
        print_red "POD_NAME is missing: Please privide value!" 1
        exit 1
    fi

    if [ -z "$REPLICAS" ]; then
        print_red "REPLICAS is missing: Please privide value!" 1
        exit 1
    fi

    KAFKA_BROKER_SET_MEMBERS=$(
      awk -v controller_replicas="$CONTROLLER_REPLICAS" -v replicas="$REPLICAS" -v node_name="$POD_NAME" -v service_name="$SERVICE_NAME" -v namespace="$NAMESPACE" '
        BEGIN {
            sep = ""

            for (i = 0; i < replicas; i++) {
                next_pod_number = controller_replicas + 1 + i
                printf("%s%d@%s-%d.%s.%s.svc.cluster.local:9093", sep, next_pod_number , node_name, i, service_name, namespace)
                sep = ","
            }
        }
      '
    )

    print_green "KAFKA_CONTROLLER_SET_MEMBERS: $KAFKA_CONTROLLER_SET_MEMBERS" 1
    print_green "KAFKA_CONTROLLER_SET_MEMBERS: $KAFKA_CONTROLLER_SET_MEMBERS"
    print_green "KAFKA_BROKER_SET_MEMBERS: $KAFKA_BROKER_SET_MEMBERS"
    print_green "KAFKA_NODE_ID: $KAFKA_NODE_ID"
    print_green "POD_NAME: $POD_NAME"

    export KAFKA_BROKER_SET_MEMBERS
    export KAFKA_NODE_ID
    export POD_NAME
  commando_controller_members.sh: |-
    #!/bin/sh

    set -e

    KAFKA_NODE_ID=$(( ${POD_NAME_INDEX##*-} + 1 ))
    POD_NAME="${POD_NAME_INDEX%-[0-9]*}"

    # POD_NAME CHECKER
    if [ -z "$POD_NAME" ]; then
        print_red "POD_NAME is missing: Please privide value!" 1
        exit 1
    fi

    # SERVICE_NAME CHECKER
    if [ -z "$SERVICE_NAME" ]; then
        print_red "SERVICE_NAME is missing: Please privide value!" 1
        exit 1
    fi

    # NAMESPACE CHECKER
    if [ -z "$NAMESPACE" ]; then
        print_red "NAMESPACE is missing: Please privide value!" 1
        exit 1
    fi

    # REPLICAS CHECKER
    if [ -z "$REPLICAS" ]; then
        print_red "REPLICAS is missing: Please privide value!" 1
        exit 1
    fi

    KAFKA_CONTROLLER_SET_MEMBERS=$(
      awk -v replicas="$REPLICAS" -v node_name="$POD_NAME" -v service_name="$SERVICE_NAME" -v namespace="$NAMESPACE" '
        BEGIN {
            sep = ""
            for (i = 0; i < replicas; i++) {
                printf("%s%d@%s-%d.%s.%s.svc.cluster.local:9093", sep, i+1, node_name, i, service_name, namespace)
                sep = ","
            }
        }
      '
    )

    export KAFKA_CONTROLLER_SET_MEMBERS
    export KAFKA_NODE_ID
    export POD_NAME
  controller.properties: |
    # Licensed to the Apache Software Foundation (ASF) under one or more
    # contributor license agreements.  See the NOTICE file distributed with
    # this work for additional information regarding copyright ownership.
    # The ASF licenses this file to You under the Apache License, Version 2.0
    # (the "License"); you may not use this file except in compliance with
    # the License.  You may obtain a copy of the License at
    #
    #    http://www.apache.org/licenses/LICENSE-2.0
    #
    # Unless required by applicable law or agreed to in writing, software
    # distributed under the License is distributed on an "AS IS" BASIS,
    # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    # See the License for the specific language governing permissions and
    # limitations under the License.

    ############################# Server Basics #############################

    # The role of this server. Setting this puts us in KRaft mode
    # Must be |process.roles=controller|
    process.roles=((KAFKA_PROCESS_ROLES))

    # The node id associated with this instance's roles
    node.id=((KAFKA_NODE_ID))

    # Information about the KRaft controller quorum.
    # Uncomment controller.quorum.voters to use a static controller quorum.
    #controller.quorum.voters=1@localhost:9093
    controller.quorum.bootstrap.servers=((KAFKA_CONTROLLER_QUORUM_BOOTSTRAP_SERVERS))

    ############################# Socket Server Settings #############################

    # The address the socket server listens on.
    # Note that only the controller listeners are allowed here when `process.roles=controller`
    #   FORMAT:
    #     listeners = listener_name://host_name:port
    #   EXAMPLE:
    #     listeners = PLAINTEXT://your.host.name:9092
    listeners=((KAFKA_LISTENERS))

    # Listener name, hostname and port the controller will advertise to admin clients, broker nodes and controller nodes.
    # Note that the only controller listeners are allowed here when `process.roles=controller`.
    # If not set, it uses the value for "listeners".
    advertised.listeners=((KAFKA_ADVERTISED_LISTENERS))

    # A comma-separated list of the names of the listeners used by the controller.
    # This is required if running in KRaft mode.
    controller.listener.names=((KAFKA_CONTROLLER_LISTENER_NAMES))

    # Maps listener names to security protocols, the default is for them to be the same. See the config documentation for more details
    #listener.security.protocol.map=PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL

    ############################# Log Basics #############################

    # A comma separated list of directories under which to store log files
    log.dirs=((KAFKA_LOG_DIR))
  controller_server_properties.sh: |-
    #!/bin/sh

    set -e

    generate_controller_server_file() {
        if [ "$KAFKA_PROCESS_ROLES" = "controller" ]; then
            sed -e "s#((KAFKA_NODE_ID))#$KAFKA_NODE_ID#g" \
                -e "s#((KAFKA_LOG_DIR))#$KAFKA_LOG_DIR#g" \
                -e "s#((KAFKA_LISTENERS))#$KAFKA_LISTENERS#g" \
                -e "s#((KAFKA_PROCESS_ROLES))#$KAFKA_PROCESS_ROLES#g" \
                -e "s#((KAFKA_CONTROLLER_LISTENER_NAMES))#$KAFKA_CONTROLLER_LISTENER_NAMES#g" \
                -e "s#((KAFKA_CLUSTER_ID))#$KAFKA_CLUSTER_ID#g" \
                -e "s#((KAFKA_CONTROLLER_QUORUM_VOTERS))#$KAFKA_CONTROLLER_QUORUM_VOTERS#g" \
                -e "s#((KAFKA_ADVERTISED_LISTENERS))#$KAFKA_ADVERTISED_LISTENERS#g" \
                -e "s#((KAFKA_CONTROLLER_QUORUM_BOOTSTRAP_SERVERS))#$KAFKA_CONTROLLER_QUORUM_BOOTSTRAP_SERVERS#g" \
                "$USER_CONFIG_DIR/commando/controller.properties" \
                > "$USER_CONFIG_DIR/server.properties"
        fi
    }
  debug.sh: |
    #!/bin/sh

    set -e

    debug() {
        if [ -n "$DEBUG" ]; then

            print_yellow "KAFKA_NODE_ID: ${KAFKA_NODE_ID}" 1
            print_yellow "KAFKA_LISTENERS: ${KAFKA_LISTENERS}" 1
            print_yellow "KAFKA_PROCESS_ROLES: ${KAFKA_PROCESS_ROLES}" 1
            print_yellow "KAFKA_CONTROLLER_LISTENER_NAMES: ${KAFKA_CONTROLLER_LISTENER_NAMES}" 1
            print_yellow "KAFKA_CLUSTER_ID: ${KAFKA_CLUSTER_ID}" 1
            print_yellow "KAFKA_CONTROLLER_QUORUM_BOOTSTRAP_SERVERS: ${KAFKA_CONTROLLER_QUORUM_BOOTSTRAP_SERVERS}" 1
            print_yellow "KAFKA_ADVERTISED_LISTENERS: ${KAFKA_ADVERTISED_LISTENERS}" 1
            print_yellow "KAFKA_INITIAL_CONTROLLERS: ${KAFKA_INITIAL_CONTROLLERS}" 1
            print_yellow "KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR :${KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR}" 1
            print_yellow "KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR :${KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR}" 1
            print_yellow "KAFKA_TRANSACTION_STATE_LOG_MIN_ISR :${KAFKA_TRANSACTION_STATE_LOG_MIN_ISR}" 1
            print_yellow "KAFKA_NUM_NETWORK_THREADS :${KAFKA_NUM_NETWORK_THREADS}" 1
            print_yellow "KAFKA_NUM_IO_THREADS :${KAFKA_NUM_IO_THREADS}" 1
            print_yellow "KAFKA_SOCKET_SEND_BUFFER_BYTES :${KAFKA_SOCKET_SEND_BUFFER_BYTES}" 1
            print_yellow "KAFKA_RECIEVE_BUFFER_BYTES :${KAFKA_RECIEVE_BUFFER_BYTES}" 1
            print_yellow "KAFKA_REQUEST_MAX_BYTES :${KAFKA_REQUEST_MAX_BYTES}" 1
            print_yellow "KAFKA_NUM_PARTITIONS :${KAFKA_NUM_PARTITIONS}" 1
            print_yellow "KAFKA_RECOVERY_THREADS_PER_DATA_DIR :${KAFKA_RECOVERY_THREADS_PER_DATA_DIR}" 1
            print_yellow "KAFKA_SHARE_COORDINATOR_STATE_TOPIC_REPLICATION_FACTOR :${KAFKA_SHARE_COORDINATOR_STATE_TOPIC_REPLICATION_FACTOR}" 1
            print_yellow "KAFKA_SHARE_COORDINATOR_STATE_TOPIC_MIN_ISR :${KAFKA_SHARE_COORDINATOR_STATE_TOPIC_MIN_ISR}" 1
            print_yellow "KAFKA_RETENTION_HOURS :${KAFKA_RETENTION_HOURS}" 1
            print_yellow "KAFKA_RETENTION_CHECK_INTERVAL_MS :${KAFKA_RETENTION_CHECK_INTERVAL_MS}" 1
            print_yellow "KAFKA_LOG_SEGMENT_BYTES :${KAFKA_LOG_SEGMENT_BYTES}" 1
            print_yellow "KAFKA_CONTROLLER_QUORUM_VOTERS :${KAFKA_CONTROLLER_QUORUM_VOTERS}" 1

            cat "$USER_CONFIG_DIR/server.properties"
        fi
    }
  default_values.sh: "#!/bin/sh\n\nset -e\n\nKAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=${KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR:-1}\nKAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR=${KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR:-1}\nKAFKA_TRANSACTION_STATE_LOG_MIN_ISR=${KAFKA_TRANSACTION_STATE_LOG_MIN_ISR:-1}\nKAFKA_NUM_NETWORK_THREADS=${KAFKA_NUM_NETWORK_THREADS:-3}\nKAFKA_NUM_IO_THREADS=${KAFKA_NUM_IO_THREADS:-8}\nKAFKA_SOCKET_SEND_BUFFER_BYTES=${KAFKA_SOCKET_SEND_BUFFER_BYTES:-102400}\nKAFKA_RECIEVE_BUFFER_BYTES=${KAFKA_RECIEVE_BUFFER_BYTES:-102400}\nKAFKA_REQUEST_MAX_BYTES=${KAFKA_REQUEST_MAX_BYTES:-104857600}\nKAFKA_NUM_PARTITIONS=${KAFKA_NUM_PARTITIONS:-1}\nKAFKA_RECOVERY_THREADS_PER_DATA_DIR=${KAFKA_RECOVERY_THREADS_PER_DATA_DIR:-1}\nKAFKA_SHARE_COORDINATOR_STATE_TOPIC_REPLICATION_FACTOR=${KAFKA_SHARE_COORDINATOR_STATE_TOPIC_REPLICATION_FACTOR:-1}\nKAFKA_SHARE_COORDINATOR_STATE_TOPIC_MIN_ISR=${KAFKA_SHARE_COORDINATOR_STATE_TOPIC_MIN_ISR:-1}\nKAFKA_RETENTION_HOURS=${KAFKA_RETENTION_HOURS:-168}\nKAFKA_RETENTION_CHECK_INTERVAL_MS=${KAFKA_RETENTION_CHECK_INTERVAL_MS:-300000}\nKAFKA_LOG_SEGMENT_BYTES=${KAFKA_LOG_SEGMENT_BYTES:-1073741824}\n\n#
    The Kafka configuration parameter offsets.topic.replication.factor controls how
    many replicas are created for the __consumer_offsets internal topic in Kafka.\n#
    Kafka uses a special internal topic called __consumer_offsets to store the committed
    offsets of consumer groups — i.e., \n# the positions in topics that each consumer
    group has read up to. This is essential for fault tolerance, rebalancing, and
    at-least-once delivery semantics.\nexport KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR\n\n#
    The Kafka configuration parameter transaction.state.log.replication.factor controls
    \n# how many replicas Kafka will create for the internal topic that stores transactional
    state, \n# which is crucial when using Kafka transactions (i.e., exactly-once
    semantics).\n\n# Defines how many replicas each partition of the __transaction_state
    topic should have.\n# A replication factor >1 ensures availability and fault-tolerance
    for the transactional system.\nexport KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR\n\n#
    The Kafka config parameter transaction.state.log.min.isr sets the minimum number
    of in-sync replicas (ISRs) \n# that must be available for Kafka to commit writes
    to the internal __transaction_state topic, which is used for managing transactional
    state.\n# It defines the minimum number of replicas that must be in-sync (i.e.,
    caught up with the leader) for a write to be accepted on the __transaction_state
    topic.\nexport KAFKA_TRANSACTION_STATE_LOG_MIN_ISR\n\n# The Kafka broker configuration
    num.network.threads sets the number of threads Kafka uses to handle network requests—specifically,
    \n# it controls how many threads Kafka uses to:\n\n# Accept client connections\n#
    Read requests (produce, fetch, metadata, etc.)\n# Send responses\nexport KAFKA_NUM_NETWORK_THREADS\n\n#
    The number of threads that the server uses for processing requests, which may
    include disk I/O\nexport KAFKA_NUM_IO_THREADS\n\n# The send buffer (SO_SNDBUF)
    used by the socket server\nexport KAFKA_SOCKET_SEND_BUFFER_BYTES\n\n# The receive
    buffer (SO_RCVBUF) used by the socket server\nexport KAFKA_RECIEVE_BUFFER_BYTES\n\n#
    The maximum size of a request that the socket server will accept (protection against
    OOM)\nexport KAFKA_REQUEST_MAX_BYTES\n\n# The default number of log partitions
    per topic. More partitions allow greater\n# parallelism for consumption, but this
    will also result in more files across\n# the brokers.\nexport KAFKA_NUM_PARTITIONS\n\n#
    The number of threads per data directory to be used for log recovery at startup
    and flushing at shutdown.\n# This value is recommended to be increased for installations
    with data dirs located in RAID array.\nexport KAFKA_RECOVERY_THREADS_PER_DATA_DIR\n\n#
    \ Kafka configuration property used by Debezium or other Kafka-based data-sharing
    frameworks that rely on shared coordination topics.\n# This tells Kafka to replicate
    the shared coordinator state topic across N brokers, ensuring high availability.\nexport
    KAFKA_SHARE_COORDINATOR_STATE_TOPIC_REPLICATION_FACTOR\n\n# The minimum number
    of in-sync replicas (ISR) required for the coordinator state topic to accept writes.\n#
    It’s typically used in systems like Debezium, Kafka Connect, or any Kafka-based
    coordination mechanism that creates internal topics for coordination or metadata
    sharing.\nexport KAFKA_SHARE_COORDINATOR_STATE_TOPIC_MIN_ISR\n\n# The minimum
    age of a log file to be eligible for deletion due to age\nexport KAFKA_RETENTION_HOURS\n\n#
    The interval at which log segments are checked to see if they can be deleted according\n#
    to the retention policies\nexport KAFKA_RETENTION_CHECK_INTERVAL_MS\n\n# The maximum
    size of a log segment file. When this size is reached a new log segment will be
    created.\nexport KAFKA_LOG_SEGMENT_BYTES"
  environment_checker.sh: |-
    #!/bin/sh

    set -e

    environment_checker() {

        if [ -z "$KAFKA_CLUSTER_ID" ]; then
            print_red "KAFKA_CLUSTER_ID is not defined!" 1
            exit 1
        fi

        # NODE ID CHECKER
        if [ -z "$KAFKA_NODE_ID" ]; then
            print_red "KAFKA_NODE_ID is missing: Must be numeric value e.g: 1,2,3,4,5,6..." 1
            exit 1
        elif ! echo "$KAFKA_NODE_ID" | grep -Eq '^[0-9]+$'; then
            print_red "KAFKA_NODE_ID must be a numeric value e.g: 1,2,3,4,5,6..." 1
            exit 1
        elif [ "$KAFKA_NODE_ID" -le 0 ]; then
            print_red "KAFKA_NODE_ID must be greater than 0" 1
            exit 1
        fi

        # KAFKA_LISTENERS CHECKER
        if [ -z "$KAFKA_LISTENERS" ]; then
            print_red "KAFKA_LISTENERS is missing: Please privide value: e.g: CONTROLLER://0.0.0.0:9093 or BROKER://0.0.0.0:9093" 1
            exit 1
        fi

        # KAFKA_LISTENERS CHECKER
        if [ "$KAFKA_PROCESS_ROLES" != "broker" ] && [ "$KAFKA_PROCESS_ROLES" != "controller" ]; then
        print_red "Invalid KAFKA_PROCESS_ROLES: Must be either 'broker' or 'controller e.g: KAFKA_PROCESS_ROLES = broker'" 1
        exit 1
        fi


        # KAFKA_CONTROLLER_LISTENER_NAMES CHECKER
        if [ -z "$KAFKA_CONTROLLER_LISTENER_NAMES" ]; then
        print_red "KAFKA_CONTROLLER_LISTENER_NAMES is missing: Must be string value e.g: CONTROLLER or MYAPPCONTROLLER or BROKER or MYAPPBROKER ..." 1
        exit 1
        fi


        # KAFKA_CLUSTER_ID CHECKER
        if [ -z "$KAFKA_CLUSTER_ID" ]; then
        print_red "KAFKA_CLUSTER_ID is missing: Must be random string value of 22 chars string e.g: q8e655wHON-6FzkuXP4iZA" 1
        exit 1
        fi


        # Must have the set of members like: 1@[dns_controller]:[port], 2@[dns_controller]:[port], 3@[dns_controller]:[port] ...
        if [ -z "$KAFKA_CONTROLLER_SET_MEMBERS" ]; then
            print_red "KAFKA_CONTROLLER_SET_MEMBERS is invalid!" 1
            exit 1
        fi


        # GENERATE THE KAFKA_ADVERTISED_LISTENERS
        KAFKA_ADVERTISED_LISTENERS=""

        # GENERATE THE KAFKA_ADVERTISED_LISTENERS
        # FOR CONTROLLER
        if [ "$KAFKA_PROCESS_ROLES" = "controller" ]; then

            # Loop through the list and find the matching entry
            for item in $(split_string "$KAFKA_CONTROLLER_SET_MEMBERS" ","); do
                node_id="${item%%@*}"
                if [ "$node_id" = "$KAFKA_NODE_ID" ]; then
                    value="${item#*@}"
                    KAFKA_ADVERTISED_LISTENERS="CONTROLLER://$value"
                fi
            done
        fi

        # GENERATE THE KAFKA_ADVERTISED_LISTENERS
        # FOR BROKERS
        if [ "$KAFKA_PROCESS_ROLES" = "broker" ]; then

            CONTROLLER_REPLICAS=${CONTROLLER_REPLICAS:-0}
            ACTUAL_BROKER_NODE_N=$(( KAFKA_NODE_ID + CONTROLLER_REPLICAS ))

            KAFKA_ADVERTISED_LISTENERS=$(
                awk -v broker_node_n="$ACTUAL_BROKER_NODE_N" -v kafka_broker_members="$KAFKA_BROKER_SET_MEMBERS" '
                BEGIN {
                    n = split(kafka_broker_members, member_set, ",")
                    for (i = 1; i <= n; i++) {
                        split(member_set[i], parts, "@")
                        node_id = parts[1]
                        address = parts[2]

                        if (node_id == broker_node_n) {
                            print "PLAINTEXT://" address
                            exit
                        }
                    }
                }
                '
            )

            if [ -z "$KAFKA_ADVERTISED_LISTENERS" ]; then
                print_red "Can't determinate the advert. listener" 1
                exit 1
            fi
        fi


        # SET THE CORRECT NODE ID IF THE ROLE IS BROKER
        if [ "$KAFKA_PROCESS_ROLES" = "broker" ]; then

            set -- $(split_string "$KAFKA_CONTROLLER_SET_MEMBERS" ",")
            CONTROLLERS_COUNT=$#

            KAFKA_NODE_ID=$((CONTROLLERS_COUNT + KAFKA_NODE_ID))
        fi

        # GET THE SET OF COMMA SEPARATED CONTROLLER DNS
        KAFKA_INITIAL_CONTROLLERS=$(
        awk -F',' '
            BEGIN {

                split("'"$KAFKA_CONTROLLER_SET_MEMBERS"'", members, ",")
                split("'"$KAFKA_CONTROLLER_UUIDs"'", uuids, ",")

                for (i in members) {
                    split(uuids[i], u_parts, "@")
                    out = members[i] ":" u_parts[2]
                    printf("%s%s", sep, out)
                    sep = ","
                }
            }
        '
        )

        # Information about the KRaft controller quorum.
        KAFKA_CONTROLLER_QUORUM_BOOTSTRAP_SERVERS="$(echo "$KAFKA_CONTROLLER_SET_MEMBERS" | sed 's/[^,]*@//g')"

        export KAFKA_INITIAL_CONTROLLERS
        export KAFKA_CONTROLLER_QUORUM_BOOTSTRAP_SERVERS
    }
  global_tools.sh: |-
    #!/bin/sh

    set -e

    # Green Printer
    # =============
    print_green() {
        TEXT=$1
        BORDER=${2:-0}

        if [ "$BORDER" -eq 1 ]; then
            printf "\n" >&2
            printf "\033[0;32m %s \033[0m" "=====================================" >&2
            printf "\n" >&2
            printf "\033[0;32m %s \033[0m" "$TEXT" >&2
            printf "\n" >&2
            printf "\033[0;32m %s \033[0m" "=====================================" >&2
            printf "\n" >&2
        else
            printf "\n" >&2
            printf "\033[0;32m %s \033[0m" "$TEXT" >&2
            printf "\n" >&2
        fi
    }

    # Yellow Printer
    # =============
    print_yellow() {
        TEXT=$1
        BORDER=${2:-0}

        if [ "$BORDER" -eq 1 ]; then
            printf "\n" >&2
            printf "\033[0;33m %s \033[0m" "=====================================" >&2
            printf "\n" >&2
            printf "\033[0;33m %s \033[0m" "$TEXT" >&2
            printf "\n" >&2
            printf "\033[0;33m %s \033[0m" "=====================================" >&2
            printf "\n" >&2
        else
            printf "\n" >&2
            printf "\033[0;33m %s \033[0m" "$TEXT" >&2
            printf "\n" >&2
        fi
    }

    # Red Printer
    # =============
    print_red() {
        TEXT=$1
        BORDER=${2:-0}

        if [ "$BORDER" -eq 1 ]; then
            printf "\n" >&2
            printf "\033[0;31m %s \033[0m" "=====================================" >&2
            printf "\n" >&2
            printf "\033[0;31m %s \033[0m" "$TEXT" >&2
            printf "\n" >&2
            printf "\033[0;31m %s \033[0m" "=====================================" >&2
            printf "\n" >&2
        else
            printf "\n" >&2
            printf "\033[0;31m %s \033[0m" "$TEXT" >&2
            printf "\n" >&2
        fi
    }

    split_string() {
        input="$1"
        separator="$2"
        old_IFS="$IFS"
        IFS="$separator"
        set -- $input
        IFS="$old_IFS"

        # Print space-separated items
        for s in "$@"; do
            echo "$s"
        done
    }
  include.sh: |-
    #!/bin/sh

    set -e

    . "${USER_CONFIG_DIR}/commando/global_tools.sh"

    if [ "$KAFKA_PROCESS_ROLES" = "controller" ]; then
        . "${USER_CONFIG_DIR}/commando/commando_controller_members.sh"
    fi

    if [ "$KAFKA_PROCESS_ROLES" = "broker" ]; then
        . "${USER_CONFIG_DIR}/commando/commando_broker_members.sh"
    fi

    . "${USER_CONFIG_DIR}/commando/environment_checker.sh"

    . "${USER_CONFIG_DIR}/commando/default_values.sh"

    . "${USER_CONFIG_DIR}/commando/init_kafka.sh"

    . "${USER_CONFIG_DIR}/commando/controller_server_properties.sh"

    . "${USER_CONFIG_DIR}/commando/broker_server_properties.sh"

    . "${USER_CONFIG_DIR}/commando/debug.sh"
  init_kafka.sh: |-
    #!/bin/sh

    set -e

    run_kafka() {

        print_green "Starting Kafka server" 1

        if [ -n "$DEBUG" ]; then
            print_green "Settings" 1
            print_yellow "/opt/kafka/bin/kafka-storage.sh format --config ${USER_CONFIG_DIR}/server.properties -cluster-id ${KAFKA_CLUSTER_ID} --initial-controllers ${KAFKA_INITIAL_CONTROLLERS}"
        fi

        if [ ! -f "/var/lib/kafka/data/meta.properties" ]; then
            /opt/kafka/bin/kafka-storage.sh format \
                --config "${USER_CONFIG_DIR}/server.properties" \
                --cluster-id "$KAFKA_CLUSTER_ID" \
                --initial-controllers "$KAFKA_INITIAL_CONTROLLERS"
        fi

        sleep 3

        print_green "Starting Kafka server.properties" 1

        if [ ! -f "${USER_CONFIG_DIR}/server.properties" ]; then
            print_red "ERROR: Kafka config not found at ${USER_CONFIG_DIR}/server.properties" 1
            exit 1
        fi

        exec /opt/kafka/bin/kafka-server-start.sh "${USER_CONFIG_DIR}/server.properties"
    }
  timestamp: "2025-07-13T14:52:36Z"
kind: ConfigMap
metadata:
  name: '{{ .Values.configMapName }}'
  namespace: '{{ .Values.namespace }}'
