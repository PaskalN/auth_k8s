apiVersion: v1
data:
  broker.properties: |
    # Licensed to the Apache Software Foundation (ASF) under one or more
    # contributor license agreements.  See the NOTICE file distributed with
    # this work for additional information regarding copyright ownership.
    # The ASF licenses this file to You under the Apache License, Version 2.0
    # (the "License"); you may not use this file except in compliance with
    # the License.  You may obtain a copy of the License at
    #
    #    http://www.apache.org/licenses/LICENSE-2.0
    #
    # Unless required by applicable law or agreed to in writing, software
    # distributed under the License is distributed on an "AS IS" BASIS,
    # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    # See the License for the specific language governing permissions and
    # limitations under the License.

    ############################# Server Basics #############################

    # The role of this server. Setting this puts us in KRaft mode
    # Must be |process.roles=broker|
    process.roles={{KAFKA_PROCESS_ROLES}}

    # The node id associated with this instance's roles
    node.id={{KAFKA_NODE_ID}}

    # Information about the KRaft controller quorum.
    controller.quorum.bootstrap.servers={{KAFKA_CONTROLLER_QUORUM_BOOTSTRAP_SERVERS}}

    ############################# Socket Server Settings #############################

    # The address the socket server listens on. If not configured, the host name will be equal to the value of
    # java.net.InetAddress.getCanonicalHostName(), with PLAINTEXT listener name, and port 9092.
    #   FORMAT:
    #     listeners = listener_name://host_name:port
    #   EXAMPLE:
    #     listeners = PLAINTEXT://your.host.name:9092
    listeners={{KAFKA_LISTENERS}}

    # Name of listener used for communication between brokers.
    inter.broker.listener.name=PLAINTEXT

    # Listener name, hostname and port the broker will advertise to clients.
    # If not set, it uses the value for "listeners".
    advertised.listeners={{KAFKA_ADVERTISED_LISTENERS}}

    # A comma-separated list of the names of the listeners used by the controller.
    # This is required if running in KRaft mode. On a node with `process.roles=broker`, only the first listed listener will be used by the broker.
    controller.listener.names={{KAFKA_CONTROLLER_LISTENER_NAMES}}

    # Maps listener names to security protocols, the default is for them to be the same. See the config documentation for more details
    listener.security.protocol.map=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL

    # The number of threads that the server uses for receiving requests from the network and sending responses to the network
    num.network.threads={{KAFKA_NUM_NETWORK_THREADS}}

    # The number of threads that the server uses for processing requests, which may include disk I/O
    num.io.threads={{KAFKA_NUM_IO_THREADS}}

    # The send buffer (SO_SNDBUF) used by the socket server
    socket.send.buffer.bytes={{KAFKA_SOCKET_SEND_BUFFER_BYTES}}

    # The receive buffer (SO_RCVBUF) used by the socket server
    socket.receive.buffer.bytes={{KAFKA_RECIEVE_BUFFER_BYTES}}

    # The maximum size of a request that the socket server will accept (protection against OOM)
    socket.request.max.bytes={{KAFKA_REQUEST_MAX_BYTES}}


    ############################# Log Basics #############################

    # A comma separated list of directories under which to store log files
    log.dirs={{KAFKA_LOG_DIR}}

    # The default number of log partitions per topic. More partitions allow greater
    # parallelism for consumption, but this will also result in more files across
    # the brokers.
    num.partitions={{KAFKA_NUM_PARTITIONS}}

    # The number of threads per data directory to be used for log recovery at startup and flushing at shutdown.
    # This value is recommended to be increased for installations with data dirs located in RAID array.
    num.recovery.threads.per.data.dir={{KAFKA_RECOVERY_THREADS_PER_DATA_DIR}}

    ############################# Internal Topic Settings  #############################
    # The replication factor for the group metadata internal topics "__consumer_offsets" and "__transaction_state"
    # For anything other than development testing, a value greater than 1 is recommended to ensure availability such as 3.
    offsets.topic.replication.factor={{KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR}}
    transaction.state.log.replication.factor={{KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR}}
    transaction.state.log.min.isr={{KAFKA_TRANSACTION_STATE_LOG_MIN_ISR}}

    # Share state topic settings
    share.coordinator.state.topic.replication.factor={{KAFKA_SHARE_COORDINATOR_STATE_TOPIC_REPLICATION_FACTOR}}
    share.coordinator.state.topic.min.isr={{KAFKA_SHARE_COORDINATOR_STATE_TOPIC_MIN_ISR}}

    ############################# Log Flush Policy #############################

    # Messages are immediately written to the filesystem but by default we only fsync() to sync
    # the OS cache lazily. The following configurations control the flush of data to disk.
    # There are a few important trade-offs here:
    #    1. Durability: Unflushed data may be lost if you are not using replication.
    #    2. Latency: Very large flush intervals may lead to latency spikes when the flush does occur as there will be a lot of data to flush.
    #    3. Throughput: The flush is generally the most expensive operation, and a small flush interval may lead to excessive seeks.
    # The settings below allow one to configure the flush policy to flush data after a period of time or
    # every N messages (or both). This can be done globally and overridden on a per-topic basis.

    # The number of messages to accept before forcing a flush of data to disk
    #log.flush.interval.messages=10000

    # The maximum amount of time a message can sit in a log before we force a flush
    #log.flush.interval.ms=1000

    ############################# Log Retention Policy #############################

    # The following configurations control the disposal of log segments. The policy can
    # be set to delete segments after a period of time, or after a given size has accumulated.
    # A segment will be deleted whenever *either* of these criteria are met. Deletion always happens
    # from the end of the log.

    # The minimum age of a log file to be eligible for deletion due to age
    log.retention.hours={{KAFKA_RETENTION_HOURS}}

    # A size-based retention policy for logs. Segments are pruned from the log unless the remaining
    # segments drop below log.retention.bytes. Functions independently of log.retention.hours.
    #log.retention.bytes=1073741824

    # The maximum size of a log segment file. When this size is reached a new log segment will be created.
    log.segment.bytes={{KAFKA_LOG_SEGMENT_BYTES}}

    # The interval at which log segments are checked to see if they can be deleted according
    # to the retention policies
    log.retention.check.interval.ms={{KAFKA_RETENTION_CHECK_INTERVAL_MS}}
  commando.sh: "#!/bin/sh\n\nset -e\n\nif [ \"$KAFKA_PROCESS_ROLES\" = \"controller\"
    ]; then\n    source \"${USER_CONFIG_DIR}/commando/commando_controller_members.sh\"\nfi\n\nif
    [ \"$KAFKA_PROCESS_ROLES\" = \"broker\" ]; then\n    source \"${USER_CONFIG_DIR}/commando/commando_broker_members.sh\"\nfi\n\nsplit_string()
    {\n    input=\"$1\"\n    separator=\"$2\"\n    old_IFS=\"$IFS\"\n    IFS=\"$separator\"\n
    \   set -- $input\n    IFS=\"$old_IFS\"\n\n    # Print space-separated items\n
    \   for s in \"$@\"; do\n        echo \"$s\"\n    done\n}\n\nKAFKA_ADVERTISED_LISTENERS=\"\"\n\nif
    [ -z \"$KAFKA_CLUSTER_ID\" ]; then\n    echo \"KAFKA_CLUSTER_ID is not defined!\"\n
    \   exit 1\nfi\n\n# NODE ID CHECKER\nif [ -z \"$KAFKA_NODE_ID\" ]; then\n    echo
    \"KAFKA_NODE_ID is missing: Must be numeric value e.g: 1,2,3,4,5,6...\"\n    exit
    1\nelif ! echo \"$KAFKA_NODE_ID\" | grep -Eq '^[0-9]+$'; then\n    echo \"KAFKA_NODE_ID
    must be a numeric value e.g: 1,2,3,4,5,6...\"\n    exit 1\nelif [ \"$KAFKA_NODE_ID\"
    -le 0 ]; then\n    echo \"KAFKA_NODE_ID must be greater than 0\"\n    exit 1\nfi\n\n#
    KAFKA_LISTENERS CHECKER\nif [ -z \"$KAFKA_LISTENERS\" ]; then\n    echo \"KAFKA_LISTENERS
    is missing: Please privide value: e.g: CONTROLLER://0.0.0.0:9093 or BROKER://0.0.0.0:9093\"\n
    \   exit 1\nfi\n\n# KAFKA_LISTENERS CHECKER\nif [ \"$KAFKA_PROCESS_ROLES\" !=
    \"broker\" ] && [ \"$KAFKA_PROCESS_ROLES\" != \"controller\" ]; then\n  echo \"Invalid
    KAFKA_PROCESS_ROLES: Must be either 'broker' or 'controller e.g: KAFKA_PROCESS_ROLES
    = broker'\"\n  exit 1\nfi\n\n\n# KAFKA_CONTROLLER_LISTENER_NAMES CHECKER\nif [
    -z \"$KAFKA_CONTROLLER_LISTENER_NAMES\" ]; then\n  echo \"KAFKA_CONTROLLER_LISTENER_NAMES
    is missing: Must be string value e.g: CONTROLLER or MYAPPCONTROLLER or BROKER
    or MYAPPBROKER ...\"\n  exit 1\nfi\n\n\n# KAFKA_CLUSTER_ID CHECKER\nif [ -z \"$KAFKA_CLUSTER_ID\"
    ]; then\n  echo \"KAFKA_CLUSTER_ID is missing: Must be random string value of
    22 chars string e.g: q8e655wHON-6FzkuXP4iZA\"\n  exit 1\nfi\n\n\n\n# Must have
    the set of members like: 1@[dns_controller]:[port], 2@[dns_controller]:[port],
    3@[dns_controller]:[port] ...\nif [ -z \"$KAFKA_CONTROLLER_SET_MEMBERS\" ]; then\n
    \   exit 1\nfi\n\nKAFKA_CONTROLLER_QUORUM_VOTERS=\"$KAFKA_CONTROLLER_SET_MEMBERS\"\nKAFKA_CONTROLLER_QUORUM_BOOTSTRAP_SERVERS=\"$(echo
    \"$KAFKA_CONTROLLER_SET_MEMBERS\" | sed 's/[^,]*@//g')\"\n\n\n# GENERATE THE KAFKA_ADVERTISED_LISTENERS\nif
    [ \"$KAFKA_PROCESS_ROLES\" = \"controller\" ]; then\n\n    # Loop through the
    list and find the matching entry\n    for item in $(split_string \"$KAFKA_CONTROLLER_SET_MEMBERS\"
    \",\"); do\n        node_id=\"${item%%@*}\"\n        if [ \"$node_id\" = \"$KAFKA_NODE_ID\"
    ]; then\n            value=\"${item#*@}\"\n            KAFKA_ADVERTISED_LISTENERS=\"CONTROLLER://$value\"\n
    \       fi\n    done\nfi\n\nif [ \"$KAFKA_PROCESS_ROLES\" = \"broker\" ]; then\n\n
    \   CONTROLLER_REPLICAS=${CONTROLLER_REPLICAS:-0}\n    ACTUAL_BROKER_NODE_N=$((
    KAFKA_NODE_ID + CONTROLLER_REPLICAS ))\n\n    \n\n    KAFKA_ADVERTISED_LISTENERS=$(\n
    \       awk -v broker_node_n=\"$ACTUAL_BROKER_NODE_N\" -v kafka_broker_members=\"$KAFKA_BROKER_SET_MEMBERS\"
    '\n        BEGIN {\n            n = split(kafka_broker_members, member_set, \",\")\n
    \           for (i = 1; i <= n; i++) {\n                split(member_set[i], parts,
    \"@\")\n                node_id = parts[1]\n                address = parts[2]\n\n
    \               if (node_id == broker_node_n) {\n                    print \"PLAINTEXT://\"
    address\n                    exit\n                }\n            }\n        }\n
    \       '\n    )\nfi\n\nif [ -z \"$KAFKA_ADVERTISED_LISTENERS\" ]; then\n    echo
    \"Can't determinate the advert. listener\"\n    exit 1\nfi\n\n\nif [ \"$KAFKA_PROCESS_ROLES\"
    = \"broker\" ]; then\n    set -- $(split_string \"$KAFKA_CONTROLLER_SET_MEMBERS\"
    \",\")\n    CONTROLLERS_COUNT=$#\n\n    KAFKA_NODE_ID=$((CONTROLLERS_COUNT + KAFKA_NODE_ID))\nfi\n\nKAFKA_INITIAL_CONTROLLERS=$(\n
    \ awk -F',' '\n    BEGIN {\n        split(\"'\"$KAFKA_CONTROLLER_SET_MEMBERS\"'\",
    members, \",\")\n        split(\"'\"$KAFKA_CONTROLLER_UUIDs\"'\", uuids, \",\")\n
    \       for (i in members) {\n            split(uuids[i], u_parts, \"@\")\n            out
    = members[i] \":\" u_parts[2]\n            printf(\"%s%s\", sep, out)\n            sep
    = \",\"\n        }\n    }\n  '\n)\n\nKAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=${KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR:-1}\nKAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR=${KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR:-1}\nKAFKA_TRANSACTION_STATE_LOG_MIN_ISR=${KAFKA_TRANSACTION_STATE_LOG_MIN_ISR:-1}\nKAFKA_NUM_NETWORK_THREADS=${KAFKA_NUM_NETWORK_THREADS:-3}\nKAFKA_NUM_IO_THREADS=${KAFKA_NUM_IO_THREADS:-8}\nKAFKA_SOCKET_SEND_BUFFER_BYTES=${KAFKA_SOCKET_SEND_BUFFER_BYTES:-102400}\nKAFKA_RECIEVE_BUFFER_BYTES=${KAFKA_RECIEVE_BUFFER_BYTES:-102400}\nKAFKA_REQUEST_MAX_BYTES=${KAFKA_REQUEST_MAX_BYTES:-104857600}\nKAFKA_NUM_PARTITIONS=${KAFKA_NUM_PARTITIONS:-1}\nKAFKA_RECOVERY_THREADS_PER_DATA_DIR=${KAFKA_RECOVERY_THREADS_PER_DATA_DIR:-1}\nKAFKA_SHARE_COORDINATOR_STATE_TOPIC_REPLICATION_FACTOR=${KAFKA_SHARE_COORDINATOR_STATE_TOPIC_REPLICATION_FACTOR:-1}\nKAFKA_SHARE_COORDINATOR_STATE_TOPIC_MIN_ISR=${KAFKA_SHARE_COORDINATOR_STATE_TOPIC_MIN_ISR:-1}\nKAFKA_RETENTION_HOURS=${KAFKA_RETENTION_HOURS:-168}\nKAFKA_RETENTION_CHECK_INTERVAL_MS=${KAFKA_RETENTION_CHECK_INTERVAL_MS:-300000}\nKAFKA_LOG_SEGMENT_BYTES=${KAFKA_LOG_SEGMENT_BYTES:-1073741824}\n\n#
    echo \"===============================================\"\n# echo \"===============================================\"\n\n#
    echo \"KAFKA_NODE_ID: ${KAFKA_NODE_ID}\"\n# echo \"KAFKA_LISTENERS: ${KAFKA_LISTENERS}\"\n#
    echo \"KAFKA_PROCESS_ROLES: ${KAFKA_PROCESS_ROLES}\"\n# echo \"KAFKA_CONTROLLER_LISTENER_NAMES:
    ${KAFKA_CONTROLLER_LISTENER_NAMES}\"\n# echo \"KAFKA_CLUSTER_ID: ${KAFKA_CLUSTER_ID}\"\n#
    echo \"KAFKA_CONTROLLER_QUORUM_BOOTSTRAP_SERVERS: ${KAFKA_CONTROLLER_QUORUM_BOOTSTRAP_SERVERS}\"\n#
    echo \"KAFKA_ADVERTISED_LISTENERS: ${KAFKA_ADVERTISED_LISTENERS}\"\n# echo \"KAFKA_INITIAL_CONTROLLERS:
    ${KAFKA_INITIAL_CONTROLLERS}\"\n# echo \"KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR
    :${KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR}\"\n# echo \"KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR
    :${KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR}\"\n# echo \"KAFKA_TRANSACTION_STATE_LOG_MIN_ISR
    :${KAFKA_TRANSACTION_STATE_LOG_MIN_ISR}\"\n# echo \"KAFKA_NUM_NETWORK_THREADS
    :${KAFKA_NUM_NETWORK_THREADS}\"\n# echo \"KAFKA_NUM_IO_THREADS :${KAFKA_NUM_IO_THREADS}\"\n#
    echo \"KAFKA_SOCKET_SEND_BUFFER_BYTES :${KAFKA_SOCKET_SEND_BUFFER_BYTES}\"\n#
    echo \"KAFKA_RECIEVE_BUFFER_BYTES :${KAFKA_RECIEVE_BUFFER_BYTES}\"\n# echo \"KAFKA_REQUEST_MAX_BYTES
    :${KAFKA_REQUEST_MAX_BYTES}\"\n# echo \"KAFKA_NUM_PARTITIONS :${KAFKA_NUM_PARTITIONS}\"\n#
    echo \"KAFKA_RECOVERY_THREADS_PER_DATA_DIR :${KAFKA_RECOVERY_THREADS_PER_DATA_DIR}\"\n#
    echo \"KAFKA_SHARE_COORDINATOR_STATE_TOPIC_REPLICATION_FACTOR :${KAFKA_SHARE_COORDINATOR_STATE_TOPIC_REPLICATION_FACTOR}\"\n#
    echo \"KAFKA_SHARE_COORDINATOR_STATE_TOPIC_MIN_ISR :${KAFKA_SHARE_COORDINATOR_STATE_TOPIC_MIN_ISR}\"\n#
    echo \"KAFKA_RETENTION_HOURS :${KAFKA_RETENTION_HOURS}\"\n# echo \"KAFKA_RETENTION_CHECK_INTERVAL_MS
    :${KAFKA_RETENTION_CHECK_INTERVAL_MS}\"\n# echo \"KAFKA_LOG_SEGMENT_BYTES :${KAFKA_LOG_SEGMENT_BYTES}\"\n#
    echo \"KAFKA_CONTROLLER_QUORUM_VOTERS :${KAFKA_CONTROLLER_QUORUM_VOTERS}\"\n\n#
    echo \"===============================================\"\n# echo \"===============================================\"\n\n#
    Read template and replace placeholders\nif [ \"$KAFKA_PROCESS_ROLES\" = \"controller\"
    ]; then\n    sed -e \"s#{{KAFKA_NODE_ID}}#$KAFKA_NODE_ID#g\" \\\n        -e \"s#{{KAFKA_LOG_DIR}}#$KAFKA_LOG_DIR#g\"
    \\\n        -e \"s#{{KAFKA_LISTENERS}}#$KAFKA_LISTENERS#g\" \\\n        -e \"s#{{KAFKA_PROCESS_ROLES}}#$KAFKA_PROCESS_ROLES#g\"
    \\\n        -e \"s#{{KAFKA_CONTROLLER_LISTENER_NAMES}}#$KAFKA_CONTROLLER_LISTENER_NAMES#g\"
    \\\n        -e \"s#{{KAFKA_CLUSTER_ID}}#$KAFKA_CLUSTER_ID#g\" \\\n        -e \"s#{{KAFKA_CONTROLLER_QUORUM_VOTERS}}#$KAFKA_CONTROLLER_QUORUM_VOTERS#g\"
    \\\n        -e \"s#{{KAFKA_ADVERTISED_LISTENERS}}#$KAFKA_ADVERTISED_LISTENERS#g\"
    \\\n        -e \"s#{{KAFKA_CONTROLLER_QUORUM_BOOTSTRAP_SERVERS}}#$KAFKA_CONTROLLER_QUORUM_BOOTSTRAP_SERVERS#g\"
    \\\n        \"$USER_CONFIG_DIR/commando/controller.properties\" \\\n        >
    \"$USER_CONFIG_DIR/server.properties\"\nfi\n\nif [ \"$KAFKA_PROCESS_ROLES\" =
    \"broker\" ]; then\n    sed -e \"s#{{KAFKA_NODE_ID}}#$KAFKA_NODE_ID#g\" \\\n        -e
    \"s#{{KAFKA_LOG_DIR}}#$KAFKA_LOG_DIR#g\" \\\n        -e \"s#{{KAFKA_LISTENERS}}#$KAFKA_LISTENERS#g\"
    \\\n        -e \"s#{{KAFKA_PROCESS_ROLES}}#$KAFKA_PROCESS_ROLES#g\" \\\n        -e
    \"s#{{KAFKA_CONTROLLER_LISTENER_NAMES}}#$KAFKA_CONTROLLER_LISTENER_NAMES#g\" \\\n
    \       -e \"s#{{KAFKA_CLUSTER_ID}}#$KAFKA_CLUSTER_ID#g\" \\\n        -e \"s#{{KAFKA_CONTROLLER_QUORUM_VOTERS}}#$KAFKA_CONTROLLER_QUORUM_VOTERS#g\"
    \\\n        -e \"s#{{KAFKA_ADVERTISED_LISTENERS}}#$KAFKA_ADVERTISED_LISTENERS#g\"
    \\\n        -e \"s#{{KAFKA_CONTROLLER_QUORUM_BOOTSTRAP_SERVERS}}#$KAFKA_CONTROLLER_QUORUM_BOOTSTRAP_SERVERS#g\"
    \\\n        -e \"s#{{KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR}}#${KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR}#g\"
    \\\n        -e \"s#{{KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR}}#${KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR}#g\"
    \\\n        -e \"s#{{KAFKA_TRANSACTION_STATE_LOG_MIN_ISR}}#${KAFKA_TRANSACTION_STATE_LOG_MIN_ISR}#g\"
    \\\n        -e \"s#{{KAFKA_NUM_NETWORK_THREADS}}#${KAFKA_NUM_NETWORK_THREADS}#g\"
    \\\n        -e \"s#{{KAFKA_NUM_IO_THREADS}}#${KAFKA_NUM_IO_THREADS}#g\" \\\n        -e
    \"s#{{KAFKA_SOCKET_SEND_BUFFER_BYTES}}#${KAFKA_SOCKET_SEND_BUFFER_BYTES}#g\" \\\n
    \       -e \"s#{{KAFKA_RECIEVE_BUFFER_BYTES}}#${KAFKA_RECIEVE_BUFFER_BYTES}#g\"
    \\\n        -e \"s#{{KAFKA_REQUEST_MAX_BYTES}}#${KAFKA_REQUEST_MAX_BYTES}#g\"
    \\\n        -e \"s#{{KAFKA_NUM_PARTITIONS}}#${KAFKA_NUM_PARTITIONS}#g\" \\\n        -e
    \"s#{{KAFKA_RECOVERY_THREADS_PER_DATA_DIR}}#${KAFKA_RECOVERY_THREADS_PER_DATA_DIR}#g\"
    \\\n        -e \"s#{{KAFKA_SHARE_COORDINATOR_STATE_TOPIC_REPLICATION_FACTOR}}#${KAFKA_SHARE_COORDINATOR_STATE_TOPIC_REPLICATION_FACTOR}#g\"
    \\\n        -e \"s#{{KAFKA_SHARE_COORDINATOR_STATE_TOPIC_MIN_ISR}}#${KAFKA_SHARE_COORDINATOR_STATE_TOPIC_MIN_ISR}#g\"
    \\\n        -e \"s#{{KAFKA_RETENTION_HOURS}}#${KAFKA_RETENTION_HOURS}#g\" \\\n
    \       -e \"s#{{KAFKA_RETENTION_CHECK_INTERVAL_MS}}#${KAFKA_RETENTION_CHECK_INTERVAL_MS}#g\"
    \\\n        -e \"s#{{KAFKA_LOG_SEGMENT_BYTES}}#${KAFKA_LOG_SEGMENT_BYTES}#g\"
    \\\n        \"$USER_CONFIG_DIR/commando/broker.properties\" \\\n        > \"$USER_CONFIG_DIR/server.properties\"\nfi\n\nsleep
    2\n\nif [ ! -f \"/var/lib/kafka/data/meta.properties\" ]; then\n    /opt/kafka/bin/kafka-storage.sh
    format \\\n        --config \"${USER_CONFIG_DIR}/server.properties\" \\\n        --cluster-id
    \"$KAFKA_CLUSTER_ID\" \\\n        --initial-controllers \"$KAFKA_INITIAL_CONTROLLERS\"\nfi\n\nsleep
    3\n\nif [ ! -f \"${USER_CONFIG_DIR}/server.properties\" ]; then\n    echo \"ERROR:
    Kafka config not found at ${USER_CONFIG_DIR}/server.properties\"\n    exit 1\nfi\n\nexec
    /opt/kafka/bin/kafka-server-start.sh \"${USER_CONFIG_DIR}/server.properties\""
  commando_broker_members.sh: |-
    #!/bin/sh

    set -e


    # CONTROLLER SETUP
    # POD_NAME CHECKER
    if [ -z "$POD_NAME_CONTROLLER" ]; then
        echo "POD_NAME_CONTROLLER is missing: Please privide value!"
        exit 1
    fi

    # SERVICE_NAME CHECKER
    if [ -z "$SERVICE_NAME" ]; then
        echo "SERVICE_NAME is missing: Please privide value!"
        exit 1
    fi

    # NAMESPACE CHECKER
    if [ -z "$NAMESPACE" ]; then
        echo "NAMESPACE is missing: Please privide value!"
        exit 1
    fi

    # CONTROLLER_REPLICAS CHECKER
    if [ -z "$CONTROLLER_REPLICAS" ]; then
        echo "CONTROLLER_REPLICAS is missing: Please privide value!"
        exit 1
    fi

    # CONTROLLER_SERVICE CHECKER
    if [ -z "$CONTROLLER_SERVICE" ]; then
        echo "CONTROLLER_SERVICE is missing: Please privide value!"
        exit 1
    fi

    KAFKA_CONTROLLER_SET_MEMBERS=$(
      awk -v replicas="$CONTROLLER_REPLICAS" -v node_name="$POD_NAME_CONTROLLER" -v service_name="$CONTROLLER_SERVICE" -v namespace="$NAMESPACE" '
        BEGIN {
            sep = ""
            for (i = 0; i < replicas; i++) {
                printf("%s%d@%s-%d.%s.%s.svc.cluster.local:9093", sep, i+1, node_name, i, service_name, namespace)
                sep = ","
            }
        }
      '
    )

    export KAFKA_CONTROLLER_SET_MEMBERS

    # BROKER SETUP
    KAFKA_NODE_ID=$(( ${POD_NAME_INDEX##*-} + 1 ))
    POD_NAME="${POD_NAME_INDEX%-[0-9]*}"

    if [ -z "$POD_NAME" ]; then
        echo "POD_NAME is missing: Please privide value!"
        exit 1
    fi

    if [ -z "$REPLICAS" ]; then
        echo "REPLICAS is missing: Please privide value!"
        exit 1
    fi

    KAFKA_BROKER_SET_MEMBERS=$(
      awk -v controller_replicas="$CONTROLLER_REPLICAS" -v replicas="$REPLICAS" -v node_name="$POD_NAME" -v service_name="$SERVICE_NAME" -v namespace="$NAMESPACE" '
        BEGIN {
            sep = ""

            for (i = 0; i < replicas; i++) {
                next_pod_number = controller_replicas + 1 + i
                printf("%s%d@%s-%d.%s.%s.svc.cluster.local:9093", sep, next_pod_number , node_name, i, service_name, namespace)
                sep = ","
            }
        }
      '
    )

    echo "KAFKA_CONTROLLER_SET_MEMBERS: $KAFKA_CONTROLLER_SET_MEMBERS"
    echo "KAFKA_BROKER_SET_MEMBERS: $KAFKA_BROKER_SET_MEMBERS"
    echo "KAFKA_NODE_ID: $KAFKA_NODE_ID"
    echo "POD_NAME: $POD_NAME"

    export KAFKA_BROKER_SET_MEMBERS
    export KAFKA_NODE_ID
    export POD_NAME
  commando_controller_members.sh: |-
    #!/bin/sh

    set -e

    KAFKA_NODE_ID=$(( ${POD_NAME_INDEX##*-} + 1 ))
    POD_NAME="${POD_NAME_INDEX%-[0-9]*}"

    # POD_NAME CHECKER
    if [ -z "$POD_NAME" ]; then
        echo "POD_NAME is missing: Please privide value!"
        exit 1
    fi

    # SERVICE_NAME CHECKER
    if [ -z "$SERVICE_NAME" ]; then
        echo "SERVICE_NAME is missing: Please privide value!"
        exit 1
    fi

    # NAMESPACE CHECKER
    if [ -z "$NAMESPACE" ]; then
        echo "NAMESPACE is missing: Please privide value!"
        exit 1
    fi

    # REPLICAS CHECKER
    if [ -z "$REPLICAS" ]; then
        echo "REPLICAS is missing: Please privide value!"
        exit 1
    fi

    KAFKA_CONTROLLER_SET_MEMBERS=$(
      awk -v replicas="$REPLICAS" -v node_name="$POD_NAME" -v service_name="$SERVICE_NAME" -v namespace="$NAMESPACE" '
        BEGIN {
            sep = ""
            for (i = 0; i < replicas; i++) {
                printf("%s%d@%s-%d.%s.%s.svc.cluster.local:9093", sep, i+1, node_name, i, service_name, namespace)
                sep = ","
            }
        }
      '
    )

    export KAFKA_CONTROLLER_SET_MEMBERS
    export KAFKA_NODE_ID
    export POD_NAME
  controller.properties: |
    # Licensed to the Apache Software Foundation (ASF) under one or more
    # contributor license agreements.  See the NOTICE file distributed with
    # this work for additional information regarding copyright ownership.
    # The ASF licenses this file to You under the Apache License, Version 2.0
    # (the "License"); you may not use this file except in compliance with
    # the License.  You may obtain a copy of the License at
    #
    #    http://www.apache.org/licenses/LICENSE-2.0
    #
    # Unless required by applicable law or agreed to in writing, software
    # distributed under the License is distributed on an "AS IS" BASIS,
    # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    # See the License for the specific language governing permissions and
    # limitations under the License.

    ############################# Server Basics #############################

    # The role of this server. Setting this puts us in KRaft mode
    # Must be |process.roles=controller|
    process.roles={{KAFKA_PROCESS_ROLES}}

    # The node id associated with this instance's roles
    node.id={{KAFKA_NODE_ID}}

    # Information about the KRaft controller quorum.
    # Uncomment controller.quorum.voters to use a static controller quorum.
    #controller.quorum.voters=1@localhost:9093
    controller.quorum.bootstrap.servers={{KAFKA_CONTROLLER_QUORUM_BOOTSTRAP_SERVERS}}

    ############################# Socket Server Settings #############################

    # The address the socket server listens on.
    # Note that only the controller listeners are allowed here when `process.roles=controller`
    #   FORMAT:
    #     listeners = listener_name://host_name:port
    #   EXAMPLE:
    #     listeners = PLAINTEXT://your.host.name:9092
    listeners={{KAFKA_LISTENERS}}

    # Listener name, hostname and port the controller will advertise to admin clients, broker nodes and controller nodes.
    # Note that the only controller listeners are allowed here when `process.roles=controller`.
    # If not set, it uses the value for "listeners".
    advertised.listeners={{KAFKA_ADVERTISED_LISTENERS}}

    # A comma-separated list of the names of the listeners used by the controller.
    # This is required if running in KRaft mode.
    controller.listener.names={{KAFKA_CONTROLLER_LISTENER_NAMES}}

    # Maps listener names to security protocols, the default is for them to be the same. See the config documentation for more details
    #listener.security.protocol.map=PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL

    ############################# Log Basics #############################

    # A comma separated list of directories under which to store log files
    log.dirs={{KAFKA_LOG_DIR}}
  timestamp: "2025-07-03T16:51:39Z"
kind: ConfigMap
metadata:
  name: kafka-init-scripts
  namespace: authentication
